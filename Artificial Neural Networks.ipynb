{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://d9w.github.io/deep-learning-intro/\">https://d9w.github.io/deep-learning-intro/</a><br>Based on the Supaero Data Science Deep Learning class: https://supaerodatascience.github.io/deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. [Biological neural networks](#sec1)\n",
    "2. [Why invent artificial neural networks?](#sec2)\n",
    "2. [Artificial neural networks](#sec3)\n",
    "3. [Propagating values through a network](#sec4)\n",
    "3. [Learning the weights of a neural network (regression case)](#sec5)\n",
    "4. [Neural networks for classification](#sec6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a> Biological neural networks\n",
    "\n",
    "In 1839, *Cell theory* introduces the idea that living bodies are made of cells.<br>\n",
    "**Mikroskopische Untersuchungen über die Uebereinstimmung in der Struktur und dem Wachsthum der Thiere und Pflanzen.**<br>\n",
    "Schwann, Theodor. Berlin: Sander. (1839).\n",
    "\n",
    "But due to limitations in microsopy in the XIXth century, no one had observed the basic constituents of nerve tissue and the nervous system stood as an exception to cell theory.\n",
    "\n",
    "In the first issue of the Revista Trimestral de Histología Normal y Patológica (May, 1888), Santiago Ramón y Cajal shows a physical separation between individual cells at the axon/dendrite connection (in chickens). This lays the basis of the **neuron doctrine** which lead to his 1906 Nobel prize for Physiology or Medecine.<br>\n",
    "**Neuron theory, the cornerstone of neuroscience, on the centenary of the Nobel Prize award to Santiago Ramón y Cajal**.<br>\n",
    "López-Muñoz, F., Boya, J., & Alamo, C. Brain research bulletin, 70(4-6), 391-405. (2006).\n",
    "\n",
    "Neuron doctrine:\n",
    "- neurons are the basis constituent of the nervous system\n",
    "- dendrites $\\rightarrow$ nucleus $\\rightarrow$ axon $\\rightarrow$ synapses (Law of Dynamic Polarization).\n",
    "- electrical impulses.\n",
    "- chemical neuro-transmitters.\n",
    "\n",
    "<img src=\"img/neuron_bio.png\" width=\"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id=\"sec2\"></a> Why invent \"artificial neural networks\"?\n",
    "\n",
    "Our rationale goes as follows.<br>\n",
    "The human brain processes information efficiently, can we design an artificial computing method that mimics it?<br>\n",
    "But computers process bits, so our artificial neurons should take logical values as inputs.\n",
    "\n",
    "Let's take inspiration from the biological neuron for that and make a simplified model of a neuron. Suppose an input signal in the form of a binary vector $x$. The elements of $x$ can indicate binary statements which are true or false, such as \"it's raining\" or \"I have an umbrella\". We will model the *activation* of a neuron as:\n",
    "$$f(x)=\\left\\{\\begin{array}{ll} 0 & \\textrm{if }w^T x+b\\leq 0 \\\\ 1 & \\textrm{otherwise}\\end{array}\\right.=step\\left(w^Tx + b\\right),$$\n",
    "where $step$ is Heaviside's step function.\n",
    "\n",
    "We will call such a function *Rosenblatt's Perceptron*.\n",
    "Basically, a perceptron is a linear separation rule.\n",
    "Intuitively, it is a machine that weights evidence $x$ and compares it to threshold $b$ in order to make a decision $f(x)$.\n",
    "\n",
    "Although perceptrons were invented in the 50's and are not really representative of modern artificial neural networks, manipulating them conveys some of the important intuitions about artificial networks, so we will go into a \"back to the future\" mode for the next paragraphs before going any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Back to the future, logical gates.** <br>\n",
    "Take Rosenblatt's perceptron and find input weights that correspond to AND, OR and NAND gates (for two binary inputs).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answer1\" data-toggle=\"collapse\"><b>Ready to see the answer? (click to expand)</b></a><br>\n",
    "<div id=\"answer1\" class=\"collapse\">\n",
    "\n",
    "Rosenblatt's perceptron is the function $step(w_1 x_1 + w_2 x_2 +b)$. \n",
    "\n",
    "With $(w_1, w_2, b) = (2,2,-1)$ we get an OR gate. \n",
    "\n",
    "With $(w_1, w_2, b) = (2,2, -3)$ we get an AND gate. \n",
    "\n",
    "With $(w_1, w_2, b) = (-2,-2,3)$ we get a NAND gate.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input values:\n",
      " [[0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]]\n",
      "testing OR gate: [0. 1. 1. 1.]\n",
      "testing AND gate: [0. 0. 0. 1.]\n",
      "testing NAND gate: [1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron(w,b,x):\n",
    "    return np.heaviside(np.dot(x,w)+b,0)\n",
    "\n",
    "x = np.zeros((4,2))\n",
    "x[1,0] = 1.\n",
    "x[2,1] = 1.\n",
    "x[3,0] = 1.\n",
    "x[3,1] = 1.\n",
    "\n",
    "print(\"input values:\\n\", x)\n",
    "\n",
    "def OR(x):\n",
    "    w = np.array([2.,2.])\n",
    "    b = -1.\n",
    "    return perceptron(w,b,x)\n",
    "print(\"testing OR gate:\", OR(x))\n",
    "\n",
    "def AND(x):\n",
    "    w = np.array([2.,2.])\n",
    "    b = -3.\n",
    "    return perceptron(w,b,x)\n",
    "print(\"testing AND gate:\", AND(x))\n",
    "\n",
    "def NAND(x):\n",
    "    w = np.array([-2.,-2.])\n",
    "    b = 3.\n",
    "    return perceptron(w,b,x)\n",
    "print(\"testing NAND gate:\", NAND(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Back to the future, the XOR affair.** <br> \n",
    "Does it seem possible to describe a XOR gate with a perceptron? Building a XOR function can be seen as a classification problem; what is the family of classification problems that can be tackled by perpectrons (hint: recall the beginning of the SVM class)?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answer2\" data-toggle=\"collapse\"><b>Ready to see the answer? (click to expand)</b></a><br>\n",
    "<div id=\"answer2\" class=\"collapse\">\n",
    "    \n",
    "No, it is actually not possible to model a XOR gate with a perceptron. Since perceptrons implement a threshold on a linear combination of the inputs, they can only separate (shatter, in VC theory) classes that are... linearly separable. XOR is a typical example of non-linearly separable data.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter([0,1],[0,1],c='b')\n",
    "plt.scatter([1,0],[0,1],c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Back to the future, networks of perceptrons.**<br> \n",
    "It is possible to connect perceptrons together to get a XOR function (for example by remarking that $x_1$ XOR $x_2$ = $(x_1$ OR $x_2)$ AND $(x_1$ NAND $x_2)$). It is actually possible to do so for any logical function. Such connected architectures are called Multi-Layer Perceptrons (MLP). This term was later used (abusively) for multi-layered networks of artifical neurons, regardless of their activation functions.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/xor.png\" width=\"600px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input values:\n",
      " [[0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]]\n",
      "testing XOR gate: [0. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "def XOR(x):\n",
    "    y1 = OR(x)\n",
    "    y2 = NAND(x)\n",
    "    y = np.array([y1,y2]).T\n",
    "    return AND(y)\n",
    "print(\"input values:\\n\", x)\n",
    "print(\"testing XOR gate:\", XOR(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, any logical circuit can be represented as an MLP. **This seems a great ground for computer-based intelligence!**\n",
    "\n",
    "Now the question is \"how does one find (learn) the structure and weights of a neural network that seems intelligent?\". We will come to that in a minute; let's first play around a bit with artificial neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Weight invariance of the Heaviside neuron.**<br>\n",
    "If one multiplies all weights and the bias of a perceptron by a constant $c>0$, does the logical function change?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answer3\" data-toggle=\"collapse\"><b>Ready to see the answer? (click to expand)</b></a><br>\n",
    "<div id=\"answer3\" class=\"collapse\">\n",
    "    \n",
    "A perceptron is invariant by scalar multiplication.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have considered a drastic \"0 or 1\" activation for a certain neuron. Suppose now that the input $x$ is not binary anymore: it is made of continuous variables, like a temperature, or a user preference. Then, when processing an input $x$, either the neuron's stimulation $w^Tx$ is above $-b$ or it is below. This makes the output of a neuron very sensitive to noise in the input, or to errors in setting the weights. Conversely, we could wish for a function that is *S-shaped* and that transitions smoothly from 0 to 1.\n",
    "\n",
    "An example of such a function is the sigmoid function:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1. + np.exp(-x))\n",
    "\n",
    "XX = np.arange(-10.,10.,0.1)\n",
    "plt.plot(XX,sigmoid(XX));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice: Weight invariance of the sigmoid neuron.**<br>\n",
    "Explain why the weight invariance property does not hold for sigmoid activation functions? Can we regain this invariance property when $c\\rightarrow\\infty$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answer4\" data-toggle=\"collapse\"><b>Ready to see the answer? (click to expand)</b></a><br>\n",
    "<div id=\"answer4\" class=\"collapse\">\n",
    "    \n",
    "This weight invariance property is not true for a sigmoid neuron because, for a given input $x$, multiplying all input weights by $c$ boils down to a shift by a multiplicative factor of $c$ on the horizontal scale of the sigmoid function above, which changes the value taken on the vertical axis. However, when $c\\rightarrow\\infty$, the sigmoid acts as a $step(w^Tx+b)$ function so tends to the same behavior as the perceptron.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Beyond perceptrons.** Modern ANNs model logical functions as well as classification or regression tasks. For this purpose, one needs to find the correct weights so that the network actually produces the desired relation between inputs and outputs. Automatically tuning these weights from the data is the learning algorithm we need to design.\n",
    "</div>\n",
    "\n",
    "Note that the definition above restricts learning algorithms to finding optimized weights for a predefined set of neurons and connections. This is too reductive: finding the appropriate network structure is just as important. Unfortunately, Neural Architecture Search is still a pretty open research topic and, in this class, we will focus on weight learning for predefined network topologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our progression of ideas has been:\n",
    "- The human brain processes information efficiently, can we design an artificial computing method that mimics it?\n",
    "- The biologicial neuron.\n",
    "- Rosenblatt's perceptron: $f(x)=step(\\sum w^T x+b)$ for a binary $x$ input vector.\n",
    "- Networks of artificial neurons = Artifical Neural Networks (ANN).\n",
    "- Generalization on activation functions and the particular case of the (logistic) sigmoid function.\n",
    "\n",
    "If you get all the ideas behind each of these steps, we can move on to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id=\"sec3\"></a>Artificial neural networks\n",
    "\n",
    "**The idea:**\n",
    "1. Each neuron processes a bit of information and passes it to its children.\n",
    "2. Overall the network processes raw information into general concepts.\n",
    "\n",
    "**Question:** can we mimic this system of connections in a learning system that adapts its parameters to the data it is exposed to?\n",
    "\n",
    "<img src=\"img/neuron_bio.png\" width=\"600px\"></img>\n",
    "\n",
    "**A formal neuron:**\n",
    "$$z = \\sigma(w^T x + b)$$\n",
    "- $x$ is the input (the $n$-dimensional signal received through dendrites)\n",
    "- $w$ is an $n$-dimensional vector of weights that give more or less importance to the elements of $x$\n",
    "- $b$ is a scalar bias\n",
    "- $\\sigma$ is the neuron's activation function\n",
    "- $z$ is the output (the signal along the \"axon\")\n",
    "\n",
    "Note that the formal neuron is a *function*, very different from the impulses carried by axons in biological neurons.\n",
    "\n",
    "<img src=\"img/artificial-neuron.png\" width=\"250px\">\n",
    "\n",
    "**Computation graph**\n",
    "\n",
    "A neural network is obtained by connecting some neuron's outputs to other neurons inputs. The goal of such a network is typically to learn how to imitate a certain function $f(x)$ for which we are given training data pairs $(x,y)$ with $y = f(x) + \\textrm{noise}$. Such a network has thus three types of neurons:\n",
    "- Input neurons. Those correspond to the different input variables $x_j$ describing our training examples.\n",
    "- Output neurons. Those correspond to the targets $y$ in our examples.\n",
    "- Hidden neurons. Any neurons that's not an input or an output neuron.\n",
    "\n",
    "Therefore, a neural network is a computation graph, with inputs $x$ and outputs $y$, where nodes are neurons and edges connect the output signal of a node to one of the inputs of another.\n",
    "\n",
    "**A little vocabulary:**\n",
    "- A neural network is a computation graph.\n",
    "- The input layer is composed of all input neurons.\n",
    "- A layer is a (maximum) set of unconnected neurons, at the same depth from the input layer.\n",
    "- The output layer is composed of all output neurons.\n",
    "- All layers between the input and output layers are called hidden layers.\n",
    "- A neural network organized in layers is called a feedforward NN.\n",
    "- Some neural networks are not feedforward NNs and present loops. They are called Recurrent NN.\n",
    "- A multilayer NN is often called a multilayer perceptron (for historical reasons)\n",
    "- The output of a neuron is also called its activation.\n",
    "- The vector of outputs for all neurons in a given layer is called the layer's activation.\n",
    "\n",
    "<img src=\"img/nn.png\" width=\"600px\"></img>\n",
    "\n",
    "**A bit of history:**<br>\n",
    "1943: McCulloch (neurophysiologist) and Pitts (logician) suggest a first formal model for neurons.\n",
    "> A logical calculus of the ideas immanent in nervous activity. McCulloch, W. and Pitts, W. Bulletin of Mathematical Biophysics, 5:115–133. (1943). [paper](https://link.springer.com/article/10.1007%2FBF02478259), [wikipedia](https://en.wikipedia.org/wiki/Artificial_neuron).\n",
    "\n",
    "1949: Hebb suggests dendrites are strengthened whenever they are used.\n",
    "> The Organization of Behavior. Hebb, D.O. New York: Wiley & Sons. (1949). [book](), [wikipedia](https://en.wikipedia.org/wiki/Hebbian_theory).\n",
    "\n",
    "1951: [Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) designs a network of artificial neurons.\n",
    "> [wikipedia](https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator)\n",
    "\n",
    "1958: Roseblatt designs the Perceptron, with step activation functions.\n",
    "> The perceptron: A probabilistic model for information storage and organization in the brain. Rosenblatt, F. Psychological Review, 65(6), 386-408. (1958) [wikipedia](https://en.wikipedia.org/wiki/Perceptron). [paper](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.3775)\n",
    "\n",
    "1962: Widrow and Hoff suggest the delta-rule for adapting a network weights to obtain the desired output.\n",
    "> [wikipedia](https://en.wikipedia.org/wiki/Least_mean_squares_filter)\n",
    "\n",
    "1970s and 80s: The \"quiet years\", first AI winter. Attention turns to other methods while computing resources slowly increase. The Lighthill report halts AI research in the UK.\n",
    "> Artificial Intelligence: A General Survey. James Lighthill. Artificial Intelligence: a paper symposium, UK Science Research Council. (1973).\n",
    "\n",
    "1986: Rediscovery of the backpropagation algorithm (for multilayered perceptrons).\n",
    "> Learning representations by back-propagating errors. Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. Nature. 323 (6088): 533–536. 1986. [paper](https://www.nature.com/articles/323533a0).\n",
    "\n",
    "1990s: not enough computing power, not enough data, not enough recognition (while other methods like SVMs achieve good results). Second AI winter.\n",
    "\n",
    "2000s: computing power + available data allow the training of deep multi-layered architectures (deep learning). Massive successes.\n",
    "\n",
    "<div class=\"alert alert alert-success\"> Feedforward neural networks are computational graphs where edges convey scalar values between neurons (nodes).<br>\n",
    "A neuron's output results from the application of the activation function $\\sigma$ to a linear combination of its inputs $z = \\sigma(w^T x + b)$.<br>\n",
    "The network parameters are all the neuron's input weights and biases. <br>\n",
    "A neural network is a function that transforms its inputs into outputs by value propagation in the network.<br>\n",
    "Learning a neural network consists in finding the $w$ and $b$ such that the network's output matches the function $f(x)$ that generated the data pairs $(x,y = f(x)\\textrm{+noise})$.\n",
    "</div>\n",
    "\n",
    "**Universal approximation theorem:**<br>\n",
    "If $\\sigma$ is \"S-shaped\", then with enough neurons, a single layer, feed-forward NN can approximate any continuous function to an arbitrary precision.<br>\n",
    "In other words, NN are universal approximators.<br>\n",
    "<a href=\"UniversalApproximationTheorem.pdf\">Let's take a formal look at this theorem.</a>\n",
    "\n",
    "**Activation functions**\n",
    "- step\n",
    "$$\\sigma(x) = 0 \\textrm{ if }x\\leq0\\textrm{, }1\\textrm{ otherwise}$$\n",
    "- linear\n",
    "$$\\sigma(x) = x$$\n",
    "- sigmoid or logistic (which we will consider by default for now)\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "- hyperbolic tangent\n",
    "$$\\sigma(x) = \\frac{e^{x} + e^{-x}}{e^{x} - e^{-x}}$$\n",
    "- radial basis function (useful in some specific cases like Kohonen maps)\n",
    "$$\\sigma(x) = e^{-x^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a>4. Propagating values through a network\n",
    "\n",
    "Let's define a neural network that has the following structure:\n",
    "- 2 input neurons\n",
    "- first hidden layer with 4 sigmoid neurons\n",
    "- second hidden layer with 3 sigmoid neurons\n",
    "- one ouput layer with an identity neuron\n",
    "\n",
    "<img src=\"img/nn2.png\" width=\"600px\"></img>\n",
    "\n",
    "Let's initialize its weights randomly (following a $\\mathcal{N}(0,1)$ distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2,4,3,1]\n",
    "num_layers = len(sizes)\n",
    "biases = [np.random.randn(1,y) for y in sizes[1:]]\n",
    "weights = [np.random.randn(out,inp) for inp,out in zip(sizes[:-1],sizes[1:])]\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Write a function that computes the forward propagation of the input $x=[1,2]$ through the network and returns the outputs and all intermediate activations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
