{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://d9w.github.io/deep-learning-intro/\">https://d9w.github.io/deep-learning-intro/</a><br>Based on the Supaero Data Science Deep Learning class: https://supaerodatascience.github.io/deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main factor which makes deep learning so useful for computer vision is the use of convolutions. Convolutional networks exploit the fact that the data is actually an image in the learning while decreasing the number of weights in the network. To do this, they define **convolution filters** that brush across the image. Such a filter defines a so-called **feature map** that shares the weights of the filter. The result of applying a feature map on an image is a new image of lower resolution, where each pixel is the result of the convolution of the filter with a set of pixels from the input image, as illustrated on the figure below. The Stanford class [CS231n](http://cs231n.github.io/convolutional-networks/) also has an excellent demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/convnet.gif\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Convolutional layers extract important features from previous layers, transforming the image space into a feature space where each block of neurons corresponds to a feature set rather than a group of pixels. To aggregate these features, select the most important ones, and reduce the dimensionality of our network, we'll use the **pooling** operator. Pooling is the operation of down-sampling the image by grouping together certain pixels. The most common pooling operation takes the maximum value over a certain window. Max pooling has been shown to better separate features which are rare in the data.\n",
    "\n",
    "Boureau, Y-Lan, Jean Ponce, and Yann LeCun. \"A theoretical analysis of feature pooling in visual recognition.\" Proceedings of the 27th international conference on machine learning (ICML-10). 2010. [pdf](https://www.di.ens.fr/willow/pdfs/icml2010b.pdf)\n",
    "\n",
    "<img src=\"img/maxpool.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the advantages of convolution is translational invariance: a feature can appear in different parts of an image, and the network will still detect it. Here's a simple example:\n",
    "\n",
    "<img src=\"img/invariance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When defining a convolutional layer, we define the number of channels, where a channel is one slice of neurons. Our input image has one channel - the dimensions are 28 by 28 by 1. A color image has three channels, red green and blue. Using convolution, we'll convert our image of 28 by 28 by 1 into a number of different channels, which we call feature maps. The other parameter we define is the size of the kernel - how large is the filter we're passing over the previous layer. \n",
    "\n",
    "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
    "\n",
    "The options of stride, padding, dilation, and groups are further explained in the [documentation](https://pytorch.org/docs/stable/nn.html?highlight=torch%20nn%20conv2d#torch.nn.Conv2d). [This page](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) shows an illustration of these different options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[[ 1.,  3.,  1.],\n",
      "          [-1., -4.,  4.],\n",
      "          [ 4., -1., -5.]]]])\n",
      "Weight: Parameter containing:\n",
      "tensor([[[[1., 0.],\n",
      "          [0., 1.]]]], requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "Output: tensor([[[[-3.,  7.],\n",
      "          [-2., -9.]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "data = rng.integers(-5, 5, (1, 3, 3))\n",
    "conv_W = np.array([[np.identity(2)]]) #np.ones((1, 1, 2, 2))\n",
    "conv_b = np.zeros(1)\n",
    "m = torch.nn.Conv2d(1, 1, 2, stride=1)\n",
    "m.weight = torch.nn.Parameter(torch.tensor(conv_W, dtype=torch.float))\n",
    "m.bias = torch.nn.Parameter(torch.tensor(conv_b, dtype=torch.float))\n",
    "input = torch.tensor(np.array([data]), dtype=torch.float)\n",
    "output = m(input)\n",
    "print(\"Input:\", input)\n",
    "print(\"Weight:\", m.weight)\n",
    "print(\"Bias:\", m.bias)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will change our network to have the following specifications: a convolutional layer with 32 channels and a kernel size of 3, a second convolutional layer with 64 channels and a kernel size of 3, then two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.relu(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will redefine the train and validation methods to test this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "full_trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "trainset, full_validset = torch.utils.data.random_split(full_trainset, (10000, 50000))\n",
    "validset, _ = torch.utils.data.random_split(full_validset, (1000, 49000))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_valid_predictions(net):\n",
    "    validloader = torch.utils.data.DataLoader(validset, batch_size=4, shuffle=False)\n",
    "    all_labels = np.array([])\n",
    "    predictions = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels = np.append(all_labels, labels.numpy())\n",
    "            predictions = np.append(predictions, predicted.numpy())\n",
    "    return all_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.27\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "train(net)\n",
    "y_valid, predictions = get_valid_predictions(net)\n",
    "print('Accuracy: ', accuracy_score(predictions, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Discussion</h3>\n",
    "    \n",
    "We previously defined backpropagation for feed-forward networks which used the gradient of the weighted sum and neural activation function. Can backpropagation still work with these two layer types, convolution and pooling? How? What requirement is there for the operators performed by each layer?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improving Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Just to review, training a neural network is a function of maximizing some objective function $Q$; it is a process of optimization. $Q$ can be, for example, a MSE Loss function:\n",
    "$$\n",
    "Q_i(\\theta) = (X(\\theta, i) - h_i)^2\\\\\n",
    "Q(\\theta) = \\frac{1}{n}\\sum_{i=1}^n Q_i(\\theta)\n",
    "$$\n",
    "So far, we've been using SGD for this optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = ConvNet()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, we're doing our weight ($\\theta$) update according to the following:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta^{(t+1)} \\leftarrow \\alpha\\Delta \\theta^{(t)}-\\eta\\nabla Q_i (\\theta^{(t)})\\\\\n",
    "\\theta^{(t+1)} \\leftarrow \\theta^{(t)} + \\Delta \\theta^{(t+1)}\n",
    "$$\n",
    "\n",
    "These two hyperparameters, learning rate ($\\alpha$) and momentum ($\\eta$), change how the neural network minimizes the loss and can have drastic impact on the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/SupaeroDataScience/deep-learning/raw/main/deep/img/sgd.gif\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A popular optimizer is the Adaptive Moment Estimation ([Adam](https://arxiv.org/pdf/1412.6980.pdf)) optimizer. This optimizer takes into account the recent weight changes when making a new update, storing an exponentially decaying average of past gradients to create momentum and storing an exponentially decaying average of past squared gradients to avoid diminishing learning rates.\n",
    "\n",
    "$$\n",
    "m_\\theta^{(t+1)} \\leftarrow \\beta_1 m_\\theta^{(t)} + (1-\\beta_1) \\nabla Q_i(\\theta^{(t)})\\\\\n",
    "v_\\theta^{(t+1)} \\leftarrow \\beta_2 v_\\theta^{(t)} + (1-\\beta_2)(\\nabla Q_i(\\theta^{(t)}))^2\\\\\n",
    "\\hat{m}_\\theta = \\frac{m_\\theta^{(t+1)}}{1-\\beta_1^t}\\\\\n",
    "\\hat{v}_\\theta = \\frac{v_\\theta^{(t+1)}}{1-\\beta_2^t}\\\\\n",
    "\\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\eta\\frac{\\hat{m}_\\theta}{\\sqrt{\\hat{v}_\\theta}+\\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While this optimizer can often perform better than SGD, it introduces new hyperparameter choices: $\\beta_1$ (update to $\\hat{m}$), $\\beta_2$ (update to $\\hat{v}$), and $\\epsilon$ (ratio between $\\hat{m}$ and $\\hat{v}$). However, as with SGD, the choice of hyperparameters can greatly affect the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/SupaeroDataScience/deep-learning/raw/main/deep/img/adam.gif\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have a convolutional neural network and a better optimization method, let's see if we can improve the training on FashionMNIST. We will train for 20 epochs and measure training and validation error during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def validation(net):\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "    train_history = []\n",
    "    valid_history = []\n",
    "    for epoch in range(20):\n",
    "        train_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        valid_loss = validation(net)\n",
    "        train_history.append(train_loss)\n",
    "        valid_history.append(valid_loss)\n",
    "        print('Epoch %02d: train loss %0.5f, validation loss %0.5f' % (epoch, train_loss, valid_loss))\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_train_val(train, valid):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_ylabel('Training', color=color)\n",
    "    ax1.plot(train, color=color)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation', color=color)\n",
    "    ax2.plot(valid, color=color)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=512, shuffle=True, num_workers=2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00: train loss 31.07301, validation loss 1.63908\n",
      "Epoch 01: train loss 13.82224, validation loss 1.09342\n",
      "Epoch 02: train loss 9.98887, validation loss 0.94976\n",
      "Epoch 03: train loss 8.69393, validation loss 0.74668\n",
      "Epoch 04: train loss 7.42910, validation loss 0.71746\n",
      "Epoch 05: train loss 6.87183, validation loss 0.65020\n",
      "Epoch 06: train loss 6.15007, validation loss 0.68027\n",
      "Epoch 07: train loss 5.61775, validation loss 0.64668\n",
      "Epoch 08: train loss 5.16522, validation loss 0.62450\n",
      "Epoch 09: train loss 4.49845, validation loss 0.62316\n",
      "Epoch 10: train loss 4.04737, validation loss 0.58827\n",
      "Epoch 11: train loss 3.84642, validation loss 0.63983\n",
      "Epoch 12: train loss 3.35872, validation loss 0.61238\n",
      "Epoch 13: train loss 3.01734, validation loss 0.59106\n",
      "Epoch 14: train loss 2.67947, validation loss 0.58394\n",
      "Epoch 15: train loss 2.44330, validation loss 0.62803\n",
      "Epoch 16: train loss 2.07286, validation loss 0.66674\n",
      "Epoch 17: train loss 1.66910, validation loss 0.74386\n",
      "Epoch 18: train loss 1.49629, validation loss 0.70463\n",
      "Epoch 19: train loss 1.40359, validation loss 0.71720\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "train_history, valid_history = train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val(train_history, valid_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model is training well! We might notice overfitting, which is the problem of learning the training distribution while getting further from the validation distribution. This is a problem with deep learning but there are many solutions such as dropout to combat it. Let's see how the model scores on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.87      0.86      0.86        90\n",
      "     Trouser       0.95      0.97      0.96       107\n",
      "    Pullover       0.75      0.85      0.80        93\n",
      "       Dress       0.92      0.84      0.88       107\n",
      "        Coat       0.87      0.71      0.78       118\n",
      "      Sandal       0.99      0.97      0.98       109\n",
      "       Shirt       0.64      0.78      0.70        69\n",
      "     Sneaker       0.93      0.99      0.96        86\n",
      "         Bag       0.98      0.99      0.99       102\n",
      "  Ankle boot       0.98      0.96      0.97       119\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.89      0.89      0.89      1000\n",
      "weighted avg       0.90      0.89      0.89      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_valid, predictions = get_valid_predictions(net)\n",
    "labels_text = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "print('Accuracy: ', accuracy_score(predictions, y_valid))\n",
    "print(classification_report(predictions, y_valid, target_names=labels_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
