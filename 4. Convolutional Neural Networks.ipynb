{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://d9w.github.io/deep-learning-intro/\">https://d9w.github.io/deep-learning-intro/</a><br>Based on the Supaero Data Science Deep Learning class: https://supaerodatascience.github.io/deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The main factor which makes deep learning so useful for computer vision is the use of convolutions. Convolutional networks exploit the fact that the data is actually an image in the learning while decreasing the number of weights in the network. To do this, they define **convolution filters** that brush across the image. Such a filter defines a so-called **feature map** that shares the weights of the filter. The result of applying a feature map on an image is a new image of lower resolution, where each pixel is the result of the convolution of the filter with a set of pixels from the input image, as illustrated on the figure below. The Stanford class [CS231n](http://cs231n.github.io/convolutional-networks/) also has an excellent demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/convnet.gif\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "full_trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "trainset, full_validset = torch.utils.data.random_split(full_trainset, (10000, 50000))\n",
    "validset, _ = torch.utils.data.random_split(full_validset, (1000, 49000))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " [[[-5 -4 -1]\n",
      "  [-3  4 -5]\n",
      "  [ 1 -3  0]]]\n",
      "Filter:\n",
      " [[[[1. 0.]\n",
      "   [0. 1.]]]]\n",
      "Bias: [0.]\n",
      "Output:\n",
      " [[[-1. -9.]\n",
      "  [-6.  4.]]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "data = rng.integers(-5, 5, (1, 3, 3))\n",
    "conv_W = np.array([[np.identity(2)]]) #np.ones((1, 1, 2, 2))\n",
    "conv_b = np.zeros(1)\n",
    "output = forward_convolution(conv_W, conv_b, data)\n",
    "print(\"Input:\\n\", data)\n",
    "print(\"Filter:\\n\", conv_W)\n",
    "print(\"Bias:\", conv_b)\n",
    "print(\"Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Convolutional layers extract important features from previous layers, transforming the image space into a feature space where each block of neurons corresponds to a feature set rather than a group of pixels. To aggregate these features, select the most important ones, and reduce the dimensionality of our network, we'll use the **pooling** operator. Pooling is the operation of down-sampling the image by grouping together certain pixels. The most common pooling operation takes the maximum value over a certain window. Max pooling has been shown to better separate features which are rare in the data.\n",
    "\n",
    "Boureau, Y-Lan, Jean Ponce, and Yann LeCun. \"A theoretical analysis of feature pooling in visual recognition.\" Proceedings of the 27th international conference on machine learning (ICML-10). 2010. [pdf](https://www.di.ens.fr/willow/pdfs/icml2010b.pdf)\n",
    "\n",
    "<img src=\"img/maxpool.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the advantages of convolution is translational invariance: a feature can appear in different parts of an image, and the network will still detect it. Here's a simple example:\n",
    "\n",
    "<img src=\"img/invariance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When defining a convolutional layer, we define the number of channels, where a channel is one slice of neurons. Our input image has one channel - the dimensions are 28 by 28 by 1. A color image has three channels, red green and blue. Using convolution, we'll convert our image of 28 by 28 by 1 into a number of different channels, which we call feature maps. The other parameter we define is the size of the kernel - how large is the filter we're passing over the previous layer. \n",
    "\n",
    "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
    "\n",
    "The options of stride, padding, dilation, and groups are further explained in the [documentation](https://pytorch.org/docs/stable/nn.html?highlight=torch%20nn%20conv2d#torch.nn.Conv2d). [This page](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) shows an illustration of these different options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[[-5., -4., -1.],\n",
      "          [-3.,  4., -5.],\n",
      "          [ 1., -3.,  0.]]]])\n",
      "Weight: Parameter containing:\n",
      "tensor([[[[1., 0.],\n",
      "          [0., 1.]]]], requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "Output: tensor([[[[-1., -9.],\n",
      "          [-6.,  4.]]]], grad_fn=<ThnnConv2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = torch.nn.Conv2d(1, 1, 2, stride=1)\n",
    "m.weight = torch.nn.Parameter(torch.tensor(conv_W, dtype=torch.float))\n",
    "m.bias = torch.nn.Parameter(torch.tensor(conv_b, dtype=torch.float))\n",
    "input = torch.tensor(np.array([data]), dtype=torch.float)\n",
    "output = m(input)\n",
    "print(\"Input:\", input)\n",
    "print(\"Weight:\", m.weight)\n",
    "print(\"Bias:\", m.bias)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This creates the difficult question of how large is our convolutional layer, in terms of dimensions? Let's assume a layer with the following:\n",
    "\n",
    "+ batch size $N$\n",
    "+ input layer size $H_{in}$ by $W_{in}$\n",
    "+ $C_{in}$ input channels\n",
    "+ $C_{out}$ output channels\n",
    "+ Kernel size $K$, assuming 2D for now\n",
    "\n",
    "That is to say, the previous layer's size is $(N, C_{in}, H_{in}, W_{in})$. For our image input layer, this is $(512, 1, 28, 28)$. We want to calculate the size of the convolutional layer, $(N, C_{out}, H_{out}, W_{out})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This size depends on our kernel size and the other convolutional parameters like stride and padding:\n",
    "\n",
    "$$H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (K_0 - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$$\n",
    " \n",
    "$$W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\\times (K_1 - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assuming the torch defaults of `stride=1, padding=0, dilation=1`, this simplifies to:\n",
    "\n",
    "$$H_{out} = \\left\\lfloor H_{in} - K_0 + 1\\right\\rfloor$$\n",
    " \n",
    "$$W_{out} = \\left\\lfloor W_{in} - K_1 + 1\\right\\rfloor$$\n",
    "\n",
    "Note that this is straight from the [torch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll use this calculation to make a small network with 8 feature maps and a kernel size of 4 by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = (4, 4)\n",
    "h_in = 28\n",
    "w_in = 28\n",
    "c_in = 1\n",
    "c_out = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "output_size = c_out * (h_in - k[0] + 1) * (w_in - k[1] + 1)\n",
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_predictions(net):\n",
    "    validloader = torch.utils.data.DataLoader(validset, batch_size=4, shuffle=False)\n",
    "    all_labels = np.array([])\n",
    "    predictions = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels = np.append(all_labels, labels.numpy())\n",
    "            predictions = np.append(predictions, predicted.numpy())\n",
    "    return all_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.21\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/ex6.py\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.relu(x)\n",
    "        return output\n",
    "\n",
    "net = ConvNet()\n",
    "train(net)\n",
    "y_valid, predictions = get_valid_predictions(net)\n",
    "print('Accuracy: ', accuracy_score(predictions, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Discussion</h3>\n",
    "    \n",
    "We previously defined backpropagation for feed-forward networks which used the gradient of the weighted sum and neural activation function. Can backpropagation still work with these two layer types, convolution and pooling? How? What requirement is there for the operators performed by each layer?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a id=\"sec7\">7. Improving Optimization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Just to review, training a neural network is a function of maximizing some objective function $Q$; it is a process of optimization. $Q$ can be, for example, a MSE Loss function:\n",
    "$$\n",
    "Q_i(\\theta) = (X(\\theta, i) - h_i)^2\\\\\n",
    "Q(\\theta) = \\frac{1}{n}\\sum_{i=1}^n Q_i(\\theta)\n",
    "$$\n",
    "So far, we've been using SGD for this optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = ConvNet()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, we're doing our weight ($\\theta$) update according to the following:\n",
    "\n",
    "$$\n",
    "\\Delta \\theta^{(t+1)} \\leftarrow \\alpha\\Delta \\theta^{(t)}-\\eta\\nabla Q_i (\\theta^{(t)})\\\\\n",
    "\\theta^{(t+1)} \\leftarrow \\theta^{(t)} + \\Delta \\theta^{(t+1)}\n",
    "$$\n",
    "\n",
    "These two hyperparameters, learning rate ($\\alpha$) and momentum ($\\eta$), change how the neural network minimizes the loss and can have drastic impact on the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/SupaeroDataScience/deep-learning/raw/main/deep/img/sgd.gif\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A popular optimizer is the Adaptive Moment Estimation ([Adam](https://arxiv.org/pdf/1412.6980.pdf)) optimizer. This optimizer takes into account the recent weight changes when making a new update:\n",
    "\n",
    "$$\n",
    "m_\\theta^{(t+1)} \\leftarrow \\beta_1 m_\\theta^{(t)} + (1-\\beta_1) \\nabla Q_i(\\theta^{(t)})\\\\\n",
    "v_\\theta^{(t+1)} \\leftarrow \\beta_2 v_\\theta^{(t)} + (1-\\beta_2)(\\nabla Q_i(\\theta^{(t)}))^2\\\\\n",
    "\\hat{m}_\\theta = \\frac{m_\\theta^{(t+1)}}{1-\\beta_1^t}\\\\\n",
    "\\hat{v}_\\theta = \\frac{v_\\theta^{(t+1)}}{1-\\beta_2^t}\\\\\n",
    "\\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\eta\\frac{\\hat{m}_\\theta}{\\sqrt{\\hat{v}_\\theta}+\\epsilon}\n",
    "$$\n",
    "\n",
    "While this optimizer can often perform better than SGD, it introduces new hyperparameter choices: $\\beta_1$ (update to $\\hat{m}$), $\\beta_2$ (update to $\\hat{v}$), and $epsilon$ (ratio between $\\hat{m}$ and $\\hat{v}$). Let's see the defaults for torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Adam in module torch.optim.adam:\n",
      "\n",
      "class Adam(torch.optim.optimizer.Optimizer)\n",
      " |  Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
      " |  \n",
      " |  Implements Adam algorithm.\n",
      " |  \n",
      " |  It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
      " |  The implementation of the L2 penalty follows changes proposed in\n",
      " |  `Decoupled Weight Decay Regularization`_.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float, optional): learning rate (default: 1e-3)\n",
      " |      betas (Tuple[float, float], optional): coefficients used for computing\n",
      " |          running averages of gradient and its square (default: (0.9, 0.999))\n",
      " |      eps (float, optional): term added to the denominator to improve\n",
      " |          numerical stability (default: 1e-8)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
      " |          algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
      " |          (default: False)\n",
      " |  \n",
      " |  .. _Adam\\: A Method for Stochastic Optimization:\n",
      " |      https://arxiv.org/abs/1412.6980\n",
      " |  .. _Decoupled Weight Decay Regularization:\n",
      " |      https://arxiv.org/abs/1711.05101\n",
      " |  .. _On the Convergence of Adam and Beyond:\n",
      " |      https://openreview.net/forum?id=ryQu7f-RZ\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Adam\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Args:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |          specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a dict containing all parameter groups\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False)\n",
      " |      Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As with SGD, the choice of hyperparameters heavily affects the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/SupaeroDataScience/deep-learning/raw/main/deep/img/adam.gif\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 8: change the optimizer in our training method to Adam and train one of the networks we've defined.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(net):\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    return valid_loss\n",
    "\n",
    "def train(net):\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "    train_history = []\n",
    "    valid_history = []\n",
    "    for epoch in range(30):\n",
    "        train_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        valid_loss = validation(net)\n",
    "        train_history.append(train_loss)\n",
    "        valid_history.append(valid_loss)\n",
    "        print('Epoch %02d: train loss %0.5f, validation loss %0.5f' % (epoch, train_loss, valid_loss))\n",
    "    return train_history, valid_history\n",
    "\n",
    "def plot_train_val(train, valid):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_ylabel('Training', color=color)\n",
    "    ax1.plot(train, color=color)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation', color=color)\n",
    "    ax2.plot(valid, color=color)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00: train loss 33.85257, validation loss 2.37362\n",
      "Epoch 01: train loss 21.27713, validation loss 1.60573\n",
      "Epoch 02: train loss 15.11789, validation loss 1.40705\n",
      "Epoch 03: train loss 13.44160, validation loss 1.31071\n",
      "Epoch 04: train loss 12.31940, validation loss 1.25194\n",
      "Epoch 05: train loss 11.46802, validation loss 1.20446\n",
      "Epoch 06: train loss 11.06943, validation loss 1.32215\n",
      "Epoch 07: train loss 10.51771, validation loss 1.16122\n",
      "Epoch 08: train loss 10.03889, validation loss 1.21902\n",
      "Epoch 09: train loss 8.31837, validation loss 0.88929\n",
      "Epoch 10: train loss 4.87493, validation loss 0.85632\n",
      "Epoch 11: train loss 3.94069, validation loss 0.82213\n",
      "Epoch 12: train loss 3.54448, validation loss 0.82316\n",
      "Epoch 13: train loss 3.22707, validation loss 0.84774\n",
      "Epoch 14: train loss 3.00397, validation loss 0.82090\n",
      "Epoch 15: train loss 2.98787, validation loss 0.88032\n",
      "Epoch 16: train loss 2.61488, validation loss 0.89871\n",
      "Epoch 17: train loss 2.47562, validation loss 0.96823\n",
      "Epoch 18: train loss 2.24024, validation loss 0.92257\n",
      "Epoch 19: train loss 1.68410, validation loss 0.95287\n",
      "Epoch 20: train loss 1.68258, validation loss 1.03570\n",
      "Epoch 21: train loss 1.46114, validation loss 1.06404\n",
      "Epoch 22: train loss 1.15502, validation loss 1.10541\n",
      "Epoch 23: train loss 0.99422, validation loss 1.17375\n",
      "Epoch 24: train loss 0.91270, validation loss 1.15508\n",
      "Epoch 25: train loss 0.74891, validation loss 1.21609\n",
      "Epoch 26: train loss 0.62571, validation loss 1.27614\n",
      "Epoch 27: train loss 0.50158, validation loss 1.27628\n",
      "Epoch 28: train loss 0.50096, validation loss 1.42110\n",
      "Epoch 29: train loss 0.43317, validation loss 1.39021\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_train_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-c28ba6f4a0a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplot_train_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_train_val' is not defined"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=512, shuffle=True, num_workers=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = ConvNet()\n",
    "train_history, valid_history = train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_val(train_history, valid_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
